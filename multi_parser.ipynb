{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b6034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate bitsandbytes\n",
    "!pip install beautifulsoup4 sentence_transformers ipywidgets lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d69ed9-6a78-459e-877b-5da558ab51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available(), torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9af1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Мульти-парсер новостей по темам мошенничества и афер.\n",
    "Источники: Lenta.ru, RIA Новости, Коммерсантъ, Ведомости, РБК.\n",
    "Сохраняет результаты в JSON, CSV и SQLite.\n",
    "\"\"\"\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import sqlite3\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib3.util.retry import Retry\n",
    "from urllib.parse import urljoin\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc869ac7-17ae-4c6a-ba8b-10a6368bfdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попытка загрузить библиотеку для семантической фильтрации\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import numpy as np\n",
    "    HAS_SEMANTIC = True\n",
    "except ImportError:\n",
    "    HAS_SEMANTIC = False\n",
    "    print(\n",
    "        \"[WARN] пакет 'sentence_transformers' не установлен. Семантическая фильтрация отключена.\",\n",
    "        file=sys.stderr\n",
    "    )\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=1,\n",
    "    status_forcelist=[429,500,502,503,504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"],\n",
    "    respect_retry_after_header=True\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', HTTPAdapter(max_retries=retry))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retry))\n",
    "\n",
    "Article = Dict[str, Any]\n",
    "UrlMeta = Tuple[str, str, datetime.date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217139bd-6c18-462b-ac9f-109be3bfc17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start: datetime.date, end: datetime.date):\n",
    "    print(f\"[DEBUG] Гененируем даты с {start} до {end}\")\n",
    "    cur = start\n",
    "    while cur <= end:\n",
    "        yield cur\n",
    "        cur += datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c688a813-a154-43d5-a7f6-76c0b5dee145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSource:\n",
    "    name: str = \"base\"\n",
    "    def get_urls(self, date: datetime.date) -> List[str]:\n",
    "        raise NotImplementedError\n",
    "    def parse_article(self, url: str) -> Article:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb23cf-af56-4e9e-81a6-3bd9328f2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LentaSource(BaseSource):\n",
    "    name = 'lenta.ru'\n",
    "    BASE_URL = 'https://lenta.ru'\n",
    "    ARTICLE_REGEX = re.compile(r'https?://lenta\\.ru/news/\\d{4}/\\d{2}/\\d{2}/[\\w\\-]+/?')\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"[LentaSource] Инициализация источника Lenta.ru\")\n",
    "\n",
    "        # Настраиваем сессию с retry\n",
    "        self.session = requests.Session()\n",
    "        retries = Retry(\n",
    "            total=5,                # всего попыток\n",
    "            backoff_factor=0.5,\n",
    "            status_forcelist=[500, 502, 503, 504],\n",
    "            allowed_methods=[\"GET\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        self.session.mount(\"https://\", adapter)\n",
    "        self.session.mount(\"http://\", adapter)\n",
    "\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "            )\n",
    "        })\n",
    "\n",
    "    def get_urls(self, date: datetime.date) -> List[str]:\n",
    "        from urllib.parse import urljoin\n",
    "        yyyy, mm, dd = date.strftime('%Y'), date.strftime('%m'), date.strftime('%d')\n",
    "        page = 1\n",
    "        found = set()\n",
    "\n",
    "        while True:\n",
    "            if page == 1:\n",
    "                url = f\"{self.BASE_URL}/news/{yyyy}/{mm}/{dd}/\"\n",
    "            else:\n",
    "                url = f\"{self.BASE_URL}/news/{yyyy}/{mm}/{dd}/page/{page}/\"\n",
    "\n",
    "            try:\n",
    "                resp = self.session.get(url, timeout=(3.05, 10))\n",
    "                resp.raise_for_status()\n",
    "            except Exception as e:\n",
    "                print(f\"[LentaSource] Ошибка при загрузке архива {url}: {e}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, 'lxml')\n",
    "            for a in soup.find_all('a', href=True):\n",
    "                full = a['href'] if a['href'].startswith('http') else urljoin(self.BASE_URL, a['href'])\n",
    "                if self.ARTICLE_REGEX.match(full):\n",
    "                    found.add(full)\n",
    "\n",
    "            # если есть кнопка «Дальше» — идём на следующую страницу\n",
    "            if soup.select_one(\".loadmore.js-loadmore\"):\n",
    "                page += 1\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        print(f\"[LentaSource] Всего найдено {len(found)} ссылок за {date}\")\n",
    "        return list(found)\n",
    "\n",
    "\n",
    "    def parse_article(self, url: str) -> Article:\n",
    "        try:\n",
    "            resp = self.session.get(url, timeout=(10, 15))\n",
    "            resp.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[ERROR] parsing {url}: {e}\")\n",
    "            return {'url': url, 'source': self.name, 'title': '', 'body': ''}\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, 'lxml')\n",
    "        title_el = soup.select_one('h1')\n",
    "        title = title_el.get_text(strip=True) if title_el else ''\n",
    "\n",
    "        body_el = soup.select_one('div.topic-body__content') or soup.select_one('div.l-text')\n",
    "        if body_el:\n",
    "            paras = body_el.find_all('p')\n",
    "            body = '\\n'.join(p.get_text(strip=True) for p in paras) if paras else body_el.get_text(strip=True)\n",
    "        else:\n",
    "            body = ''\n",
    "\n",
    "        return {'url': url, 'source': self.name, 'title': title, 'body': body}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66de96-8493-4306-acdc-75422388f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiaSource(BaseSource):\n",
    "    name = 'ria.ru'\n",
    "    DATE_FMT = '%Y%m%d'\n",
    "    ARTICLE_REGEX = re.compile(r'https?://ria\\.ru/\\d{8}/[\\w\\-]+\\.html')\n",
    "    TITLE_SELECTORS = ['div.article__title', 'h1.article__title']\n",
    "    BODY_SELECTOR = 'div.article__body'\n",
    "\n",
    "    def __init__(self, driver=None):\n",
    "        print(\"[RiaSource] Инициализация драйвера Selenium\")\n",
    "        self.driver = driver or self._init_webdriver()\n",
    "\n",
    "    def _init_webdriver(self):\n",
    "        opts = Options()\n",
    "        opts.add_argument('--headless')\n",
    "        opts.add_argument('--no-sandbox')\n",
    "        opts.add_argument('--disable-dev-shm-usage')\n",
    "        return webdriver.Chrome(options=opts)\n",
    "\n",
    "    def _load_full_page(self, url: str, scroll_pause=1.0,\n",
    "                        click_timeout=10, load_timeout=15) -> str:\n",
    "        print(f\"[RiaSource] Загрузка полной страницы архива: {url}\")\n",
    "        self.driver.get(url)\n",
    "        time.sleep(scroll_pause)\n",
    "        prev = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        wait_click = WebDriverWait(self.driver, click_timeout)\n",
    "        wait_load = WebDriverWait(self.driver, load_timeout)\n",
    "        while True:\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause)\n",
    "            try:\n",
    "                more = wait_click.until(EC.element_to_be_clickable((\n",
    "                    By.CSS_SELECTOR,\n",
    "                    \"div.list-more.color-btn-second-hover[data-url]\"\n",
    "                )))\n",
    "                self.driver.execute_script(\"arguments[0].click();\", more)\n",
    "                wait_load.until(lambda d: d.execute_script(\"return document.body.scrollHeight\") > prev)\n",
    "                prev = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            except TimeoutException:\n",
    "                break\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_urls(self, date: datetime.date) -> List[str]:\n",
    "        url = f'https://ria.ru/{date.strftime(self.DATE_FMT)}/'\n",
    "        print(f\"[RiaSource] Получение ссылок за {date}: {url}\")\n",
    "        html = self._load_full_page(url)\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "        links = set()\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            raw = a['href']\n",
    "            full = raw if raw.startswith('http') else urljoin('https://ria.ru', raw)\n",
    "            if self.ARTICLE_REGEX.match(full):\n",
    "                links.add(full)\n",
    "\n",
    "        print(f\"[RiaSource] Найдено {len(links)} статей за {date}\")\n",
    "        return list(links)\n",
    "\n",
    "    def parse_article(self, url: str) -> Article:\n",
    "        # пытаемся получить контент, обрабатываем 429\n",
    "        for attempt in range(1, 6):\n",
    "            resp = requests.get(url, timeout=10)\n",
    "            if resp.status_code == 429:\n",
    "                retry_after = resp.headers.get(\"Retry-After\")\n",
    "                wait = int(retry_after) if retry_after and retry_after.isdigit() else attempt * 2\n",
    "                print(f\"[RiaSource] Статус 429, ожидание {wait} сек (попытка {attempt})\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            resp.raise_for_status()\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, 'lxml')\n",
    "\n",
    "        # Заголовок\n",
    "        title = \"\"\n",
    "        for sel in self.TITLE_SELECTORS:\n",
    "            if (el := soup.select_one(sel)):\n",
    "                title = el.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "        # Тело статьи\n",
    "        body_parts: List[str] = []\n",
    "        for block in soup.select('div.article__block[data-type=\"text\"]'):\n",
    "            if (text_el := block.select_one('div.article__text')):\n",
    "                paras = text_el.find_all('p')\n",
    "                if paras:\n",
    "                    body_parts.extend(p.get_text(strip=True) for p in paras)\n",
    "                else:\n",
    "                    body_parts.append(text_el.get_text(strip=True))\n",
    "        if not body_parts:\n",
    "            if (body_el := soup.select_one(self.BODY_SELECTOR)):\n",
    "                paras = body_el.find_all('p')\n",
    "                body_parts = [p.get_text(strip=True) for p in paras] if paras else [body_el.get_text(strip=True)]\n",
    "\n",
    "        body = \"\\n\\n\".join(body_parts).strip()\n",
    "\n",
    "        # Извлекаем дату публикации из URL\n",
    "        date = None\n",
    "        m = re.match(r'https?://ria\\.ru/(\\d{8})/', url)\n",
    "        if m:\n",
    "            try:\n",
    "                date = datetime.datetime.strptime(m.group(1), self.DATE_FMT).date()\n",
    "            except ValueError:\n",
    "                date = None\n",
    "\n",
    "        print(f\"[RiaSource] Получено название: {title[:60]}\")\n",
    "        print(f\"[RiaSource] Длина статьи: {len(body)} символов\")\n",
    "\n",
    "        result: Article = {\n",
    "            'url': url,\n",
    "            'source': self.name,\n",
    "            'title': title,\n",
    "            'body': body\n",
    "        }\n",
    "        if date:\n",
    "            result['date'] = date.isoformat()\n",
    "        return result\n",
    "\n",
    "    def close(self):\n",
    "        print(\"[RiaSource] Закрытие драйвера Selenium\")\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334daf5b-6c66-4ac8-9eff-7ec1dd2d7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FincultSource(BaseSource):\n",
    "    name = 'fincult.info'\n",
    "    BASE_URL = 'https://fincult.info'\n",
    "    LIST_URL = BASE_URL + '/articles/'\n",
    "    CARD_SELECTOR = 'div.card__detail h4 a'\n",
    "    SHOW_MORE_SELECTOR = '.card-list__show-all-button'\n",
    "\n",
    "    def __init__(self, driver=None):\n",
    "        print(\"[FincultSource] Инициализация драйвера Selenium\")\n",
    "        from selenium import webdriver\n",
    "        from selenium.webdriver.chrome.options import Options\n",
    "        opts = Options()\n",
    "        opts.add_argument('--headless')\n",
    "        opts.add_argument('--no-sandbox')\n",
    "        opts.add_argument('--disable-dev-shm-usage')\n",
    "        opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "        opts.add_experimental_option('useAutomationExtension', False)\n",
    "        opts.add_argument(\n",
    "            \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "        self.driver = driver or webdriver.Chrome(options=opts)\n",
    "\n",
    "    def _load_full_list(self) -> str:\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        import time\n",
    "\n",
    "        print(f\"[FincultSource] Загрузка списка статей: {self.LIST_URL}\")\n",
    "        self.driver.get(self.LIST_URL)\n",
    "        # Ждём, когда кнопка «Показать еще» появится в DOM\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, self.SHOW_MORE_SELECTOR))\n",
    "            )\n",
    "        except:\n",
    "            print(\"[FincultSource] Кнопка 'Показать еще' не появилась —, возможно, контента мало\")\n",
    "\n",
    "        # Кликаем, пока кнопка доступна\n",
    "        while True:\n",
    "            try:\n",
    "                # Скроллим вниз, чтобы кнопка оказалась в видимой области\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                time.sleep(0.5)\n",
    "                btn = WebDriverWait(self.driver, 5).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, self.SHOW_MORE_SELECTOR))\n",
    "                )\n",
    "                print(\"[FincultSource] Нажимаем 'Показать еще'\")\n",
    "                btn.click()\n",
    "                time.sleep(1)\n",
    "            except Exception:\n",
    "                print(\"[FincultSource] Больше нет кнопки 'Показать еще'\")\n",
    "                break\n",
    "\n",
    "        return self.driver.page_source\n",
    "\n",
    "    def get_urls(self, date: datetime.date) -> List[str]:\n",
    "        html = self._load_full_list()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        links = set()\n",
    "        for a in soup.select(self.CARD_SELECTOR):\n",
    "            href = a.get('href')\n",
    "            if href:\n",
    "                full = href if href.startswith('http') else self.BASE_URL + href\n",
    "                links.add(full)\n",
    "        print(f\"[FincultSource] Найдено {len(links)} ссылок\")\n",
    "        return list(links)\n",
    "\n",
    "    def parse_article(self, url: str) -> Article:\n",
    "        from selenium.webdriver.common.by import By\n",
    "        from selenium.webdriver.support.ui import WebDriverWait\n",
    "        from selenium.webdriver.support import expected_conditions as EC\n",
    "        import time\n",
    "\n",
    "        print(f\"[FincultSource] parse_article: {url}\")\n",
    "        try:\n",
    "            # Загружаем страницу и ждём, когда появится хотя бы один параграф\n",
    "            self.driver.get(url)\n",
    "            #WebDriverWait(self.driver, 30).until(\n",
    "            #    EC.presence_of_element_located((By.CSS_SELECTOR, \"div.article__paragraph\"))\n",
    "            #)\n",
    "            WebDriverWait(self.driver, 30).until(\n",
    "                lambda d: d.find_element(By.CSS_SELECTOR, 'h1') and \n",
    "                          d.find_element(By.CSS_SELECTOR, 'div.article__paragraph')\n",
    "            )\n",
    "            # Скроллим вниз, чтобы весь контент подгрузился\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1)\n",
    "\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "\n",
    "            # Заголовок: первый h1 внутри header-контейнера или первый h1 на странице\n",
    "            hdr = soup.find('div', class_='article-header__container')\n",
    "            if hdr and hdr.find('h1'):\n",
    "                title = hdr.find('h1').get_text(strip=True)\n",
    "            else:\n",
    "                h1 = soup.find('h1')\n",
    "                title = h1.get_text(strip=True) if h1 else ''\n",
    "\n",
    "            # Тело: собираем текст из всех div.article__paragraph\n",
    "            body_parts = [\n",
    "                block.get_text(\" \", strip=True)\n",
    "                for block in soup.find_all('div', class_='article__paragraph')\n",
    "                if block.get_text(strip=True)\n",
    "            ]\n",
    "            body = \"\\n\\n\".join(body_parts).strip()\n",
    "\n",
    "            # Дата публикации\n",
    "            date_el = soup.select_one(\n",
    "                'section.article-publish-date.article__text div.article__paragraph'\n",
    "            )\n",
    "            if date_el:\n",
    "                ds = date_el.get_text(strip=True)\n",
    "                try:\n",
    "                    pub_date = datetime.datetime.strptime(ds, '%d.%m.%Y %H:%M').isoformat()\n",
    "                except:\n",
    "                    pub_date = ds\n",
    "            else:\n",
    "                pub_date = ''\n",
    "\n",
    "            return {\n",
    "                'url':    url,\n",
    "                'source': self.name,\n",
    "                'title':  title,\n",
    "                'body':   body,\n",
    "                'date':   pub_date\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[FincultSource][ERROR] {url}: {type(e).__name__}: {e}\", file=sys.stderr)\n",
    "            return {'url': url, 'source': self.name, 'title': '', 'body': '', 'date': ''}\n",
    "\n",
    "    def close(self):\n",
    "        print(\"[FincultSource] Closing Selenium driver\")\n",
    "        self.driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504dceb8-2b1e-4db9-ac12-93f1bca2de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parser(start_date: str, end_date: str, do_filter: bool = False):\n",
    "    start = dateparser.parse(start_date).date()\n",
    "    end   = dateparser.parse(end_date).date()\n",
    "    sources = [\n",
    "        LentaSource(),\n",
    "        RiaSource(),\n",
    "        FincultSource(),\n",
    "    ]\n",
    "\n",
    "    # Разбиваем задачи на «простые» (requests+BS4) и «Selenium»\n",
    "    simple_tasks: List[UrlMeta] = []\n",
    "    selenium_tasks: List[Tuple[str, BaseSource]] = []\n",
    "\n",
    "    for src in sources:\n",
    "        if hasattr(src, 'driver'):\n",
    "            # Selenium-источники: FinCult, RIA и т.п.\n",
    "            for dt in daterange(start, end):\n",
    "                try:\n",
    "                    urls = src.get_urls(dt)\n",
    "                    for u in urls:\n",
    "                        selenium_tasks.append((u, src))\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] {src.name}: {e}\", file=sys.stderr)\n",
    "        else:\n",
    "            # Простые HTTP-источники: Lenta, Kommersant, Vedomosti, RBC\n",
    "            for dt in daterange(start, end):\n",
    "                try:\n",
    "                    urls = src.get_urls(dt)\n",
    "                    for u in urls:\n",
    "                        simple_tasks.append((u, src, dt))\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] {src.name} {dt}: {e}\", file=sys.stderr)\n",
    "\n",
    "    articles: List[Article] = []\n",
    "\n",
    "    # 1) Многопоточно парсим «простые» статьи\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        future_to_meta = {\n",
    "            executor.submit(src.parse_article, url): (url, src, dt)\n",
    "            for url, src, dt in simple_tasks\n",
    "        }\n",
    "        for fut in as_completed(future_to_meta):\n",
    "            url, src, dt = future_to_meta[fut]\n",
    "            try:\n",
    "                art = fut.result()\n",
    "                # Устанавливаем дату из календарного цикла\n",
    "                art[\"date\"] = dt.isoformat()\n",
    "                articles.append(art)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] parsing {url}: {e}\", file=sys.stderr)\n",
    "\n",
    "    # 2) Последовательно парсим Selenium-источники\n",
    "    for url, src in selenium_tasks:\n",
    "        try:\n",
    "            art = src.parse_article(url)\n",
    "            # Фильтруем по дате публикации, если парсер вернул валидную дату\n",
    "            try:\n",
    "                pub = dateparser.parse(art[\"date\"]).date()\n",
    "                print(f\"ДАТА = {pub}\")\n",
    "            except:\n",
    "                pub = None\n",
    "            if pub and start <= pub <= end:\n",
    "                articles.append(art)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] parsing {url}: {e}\", file=sys.stderr)\n",
    "\n",
    "    # Закрываем все драйверы\n",
    "    for src in sources:\n",
    "        if hasattr(src, \"close\"):\n",
    "            src.close()\n",
    "\n",
    "    # Семантическая фильтрация (по желанию)\n",
    "    if do_filter and HAS_SEMANTIC:\n",
    "        print(\"[MAIN] Running semantic filter\")\n",
    "        model = SentenceTransformer(\"ai-forever/sbert_large_mt_nlu_ru\", device='cuda:0')\n",
    "        corpus = [a[\"title\"] + \" \" + a[\"body\"] for a in articles]\n",
    "        emb = model.encode(corpus, batch_size=32, convert_to_numpy=True, show_progress_bar=True)\n",
    "        anchor_texts = [\n",
    "            \"Статья о банковском мошенничестве: кредитные аферы и поддельные кредиты.\",\n",
    "            \"Новость о хищении денег со счёта через фишинг.\",\n",
    "            \"Описание схемы скимминга и кражи данных банковских карт.\"\n",
    "        ]\n",
    "        # 1. Кодируем все фразы одним вызовом\n",
    "        anchor_embs = model.encode(\n",
    "            anchor_texts,\n",
    "            batch_size=len(anchor_texts),\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # 2. Строим «композитный» вектор (mean pooling)\n",
    "        q_emb = anchor_embs.mean(axis=0)\n",
    "        sims = (emb @ q_emb) / (np.linalg.norm(emb, axis=1) * np.linalg.norm(q_emb))\n",
    "        filtered = []\n",
    "        for art, score in zip(articles, sims):\n",
    "            if score >= 0.5:\n",
    "                art[\"score\"] = float(score)\n",
    "                filtered.append(art)\n",
    "        articles = filtered\n",
    "\n",
    "    # Сохраняем результаты\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    base = f\"results_{start.strftime('%Y%m%d')}_{end.strftime('%Y%m%d')}_{timestamp}\"\n",
    "\n",
    "    with open(base + \".json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    with open(base + \".csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        if articles:\n",
    "            writer = csv.DictWriter(f, fieldnames=articles[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(articles)\n",
    "\n",
    "    print(f\"[MAIN] Finished. Files: {base}.json, {base}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81d705d3-3948-4a67-998a-986c242ce749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Мульти парсер новостей\")\n",
    "    parser.add_argument(\"--start\", required=True, help=\"Дата начала YYYY-MM-DD\")\n",
    "    parser.add_argument(\"--end\",   required=True, help=\"Дата конца   YYYY-MM-DD\")\n",
    "    parser.add_argument(\"--filter\", action=\"store_true\", help=\"Включить семантический фильтр\")\n",
    "    args = parser.parse_args()\n",
    "    run_parser(args.start, args.end, args.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9bafe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_parser(\"2025-01-15\", \"2025-01-30\", True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
